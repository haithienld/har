{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "record = False\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if (cap.isOpened() == False): \n",
    "  print(\"Unable to read camera feed\")\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "out = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "\n",
    "while(True):\n",
    "  ret, frame = cap.read()\n",
    "  k = cv2.waitKey(1)\n",
    "\n",
    "  if ret == True: \n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    # press space key to start recording\n",
    "    if k%256 == 32:\n",
    "        record = True\n",
    "\n",
    "    if record:\n",
    "        out.write(frame) \n",
    "\n",
    "    # press q key to close the program\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "  else:\n",
    "     break  \n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the networks inputs\n",
    "\n",
    "# Useful Constants\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "n_steps = 32 # 32 timesteps per series\n",
    "\n",
    "def load_X(X_path):\n",
    "    file = open(X_path, 'r')\n",
    "    X_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.split(',') for row in file\n",
    "        ]], \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    file.close()\n",
    "    blocks = int(len(X_) / n_steps)\n",
    "    \n",
    "    X_ = np.array(np.split(X_,blocks))\n",
    "\n",
    "    return X_ \n",
    "\n",
    "# Load the networks outputs\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # for 0-based indexing \n",
    "    return y_ - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,layer_num):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim,hidden_dim,layer_num,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        self.bn = nn.BatchNorm1d(32)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = self.bn(inputs)\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[307.589 162.976 319.364 205.944 293.267 204.68  285.434 250.281\n",
      "   277.616 286.729 349.276 208.539 357.095 255.523 349.362 297.17\n",
      "   297.183 290.662 307.599 351.938 308.905 401.454 329.758 291.958\n",
      "   333.701 357.132 337.546 411.953 304.994 160.291 315.459 160.269\n",
      "     0.      0.    329.752 161.651]\n",
      "  [307.589 162.976 319.364 205.944 293.267 204.68  285.434 250.281\n",
      "   277.616 286.729 349.276 208.539 357.095 255.523 349.362 297.17\n",
      "   297.183 290.662 307.599 351.938 308.905 401.454 329.758 291.958\n",
      "   333.701 357.132 337.546 411.953 304.994 160.291 315.459 160.269\n",
      "     0.      0.    329.752 161.651]]]\n",
      "(1, 2, 36)\n"
     ]
    }
   ],
   "source": [
    "only_keypoint1 = []\n",
    "only_keypoint = []\n",
    "arr1 = np.array([[307.589,162.976,319.364,205.944,293.267,204.68,285.434,250.281,277.616,286.729,349.276,208.539,357.095,255.523,349.362,297.17,297.183,290.662,307.599,351.938,308.905,401.454,329.758,291.958,333.701,357.132,337.546,411.953,304.994,160.291,315.459,160.269,0,0,329.752,161.651]])  \n",
    "only_keypoint = np.array(arr1,  dtype=np.float)\n",
    "arr2 = np.array([307.589,162.976,319.364,205.944,293.267,204.68,285.434,250.281,277.616,286.729,349.276,208.539,357.095,255.523,349.362,297.17,297.183,290.662,307.599,351.938,308.905,401.454,329.758,291.958,333.701,357.132,337.546,411.953,304.994,160.291,315.459,160.269,0,0,329.752,161.651])  \n",
    "only_keypoint=np.vstack(([only_keypoint, arr2]))\n",
    "only_keypoint1=np.array([only_keypoint],dtype=np.float)\n",
    "#only_keypoint1=np.array([only_keypoint],dtype=np.float)\n",
    "print(only_keypoint1)\n",
    "print(only_keypoint1.shape)\n",
    "#a = only_keypoint.reshape (1,32, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[307.5890, 162.9760, 319.3640,  ...,   0.0000, 329.7520, 161.6510],\n",
       "         [307.5670, 162.9790, 319.3620,  ...,   0.0000, 328.5270, 161.6550],\n",
       "         [306.2980, 162.9510, 319.3510,  ...,   0.0000, 328.4950, 161.6810],\n",
       "         ...,\n",
       "         [293.2910, 122.5340, 307.6760,  ..., 128.9530, 315.4380, 119.8840],\n",
       "         [289.3920, 140.7430, 307.6150,  ...,   0.0000, 315.3930, 139.4080],\n",
       "         [295.8480, 161.6580, 307.6280,  ..., 160.3310, 314.1120, 160.2640]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = load_X(\"demofile3.txt\")\n",
    "print(inputs.shape)\n",
    "inputs=torch.from_numpy(inputs)\n",
    "inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "n_joints = 18*2\n",
    "n_categories = 6\n",
    "n_layer = 3\n",
    "#model = torch.load('lstm_6_bn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(36, 128, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       "  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(n_joints,n_hidden,n_categories,n_layer)\n",
    "model.load_state_dict(torch.load('lstm_6_bn.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_tensor, inputs = randomTrainingExampleBatch(1,'test',0)\n",
    "#category = LABELS[int(category_tensor[0])]\n",
    "inputs = inputs.to(device)\n",
    "model.cuda()\n",
    "output = model(inputs)\n",
    "top_n, top_i = output.topk(1)\n",
    "category_i = top_i[0].item()\n",
    "category = LABELS[category_i]\n",
    "category_ii = LABELS.index(category)\n",
    "category ,category_ii,inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dem frame 0\n",
      "Dautien [[27.25 21.5  28.25 19.5  37.5  21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   15.25 21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   25.25 16.25 30.5  15.5  21.25 14.25 33.25 13.5 ]]\n",
      "SHAPE (1, 36)\n",
      "dem frame 1\n",
      "SHAPE (2, 36)\n",
      "dem frame 2\n",
      "SHAPE (3, 36)\n",
      "dem frame 3\n",
      "SHAPE (4, 36)\n",
      "dem frame 4\n",
      "SHAPE (5, 36)\n",
      "dem frame 5\n",
      "SHAPE (6, 36)\n",
      "dem frame 6\n",
      "SHAPE (7, 36)\n",
      "dem frame 7\n",
      "SHAPE (8, 36)\n",
      "dem frame 8\n",
      "SHAPE (9, 36)\n",
      "dem frame 9\n",
      "SHAPE (10, 36)\n",
      "dem frame 10\n",
      "SHAPE (11, 36)\n",
      "dem frame 11\n",
      "SHAPE (12, 36)\n",
      "dem frame 12\n",
      "SHAPE (13, 36)\n",
      "dem frame 13\n",
      "SHAPE (14, 36)\n",
      "dem frame 14\n",
      "SHAPE (15, 36)\n",
      "dem frame 15\n",
      "SHAPE (16, 36)\n",
      "dem frame 16\n",
      "SHAPE (17, 36)\n",
      "dem frame 17\n",
      "SHAPE (18, 36)\n",
      "dem frame 18\n",
      "SHAPE (19, 36)\n",
      "dem frame 19\n",
      "SHAPE (20, 36)\n",
      "dem frame 20\n",
      "SHAPE (21, 36)\n",
      "dem frame 21\n",
      "SHAPE (22, 36)\n",
      "dem frame 22\n",
      "SHAPE (23, 36)\n",
      "dem frame 23\n",
      "SHAPE (24, 36)\n",
      "dem frame 24\n",
      "SHAPE (25, 36)\n",
      "dem frame 25\n",
      "SHAPE (26, 36)\n",
      "dem frame 26\n",
      "SHAPE (27, 36)\n",
      "dem frame 27\n",
      "SHAPE (28, 36)\n",
      "dem frame 28\n",
      "SHAPE (29, 36)\n",
      "dem frame 29\n",
      "SHAPE (30, 36)\n",
      "dem frame 30\n",
      "SHAPE (31, 36)\n",
      "dem frame 31\n",
      "SHAPE (32, 36)\n",
      "SHAPEPRO (1, 32, 36)\n",
      "SHAPEPRO [[[27.25 21.5  28.25 ... 14.25 33.25 13.5 ]\n",
      "  [26.5  21.5  28.5  ... 14.5  33.25 13.5 ]\n",
      "  [26.25 21.5  28.25 ... 13.5  32.5  13.25]\n",
      "  ...\n",
      "  [26.25 21.5  27.5  ... 13.25 32.5  13.25]\n",
      "  [26.5  21.5  27.5  ... 13.25 32.5  13.5 ]\n",
      "  [26.5  21.5  27.5  ... 13.25 32.5  13.5 ]]]\n",
      "input tensor([[[27.2500, 21.5000, 28.2500,  ..., 14.2500, 33.2500, 13.5000],\n",
      "         [26.5000, 21.5000, 28.5000,  ..., 14.5000, 33.2500, 13.5000],\n",
      "         [26.2500, 21.5000, 28.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         ...,\n",
      "         [26.2500, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.2500],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.5000],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.5000]]])\n",
      "Tong hop) WAVING_2HANDS 3 tensor([[[27.2500, 21.5000, 28.2500,  ..., 14.2500, 33.2500, 13.5000],\n",
      "         [26.5000, 21.5000, 28.5000,  ..., 14.5000, 33.2500, 13.5000],\n",
      "         [26.2500, 21.5000, 28.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         ...,\n",
      "         [26.2500, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.2500],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.5000],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.5000]]],\n",
      "       device='cuda:0')\n",
      "dem frame 0\n",
      "Dautien [[26.5  21.5  27.5  19.5  37.5  21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   15.25 21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   24.25 16.25 30.25 15.5  20.25 13.5  32.5  13.5 ]]\n",
      "SHAPE (1, 36)\n",
      "dem frame 1\n",
      "SHAPE (2, 36)\n",
      "dem frame 2\n",
      "SHAPE (3, 36)\n",
      "dem frame 3\n",
      "SHAPE (4, 36)\n",
      "dem frame 4\n",
      "SHAPE (5, 36)\n",
      "dem frame 5\n",
      "SHAPE (6, 36)\n",
      "dem frame 6\n",
      "SHAPE (7, 36)\n",
      "dem frame 7\n",
      "SHAPE (8, 36)\n",
      "dem frame 8\n",
      "SHAPE (9, 36)\n",
      "dem frame 9\n",
      "SHAPE (10, 36)\n",
      "dem frame 10\n",
      "SHAPE (11, 36)\n",
      "dem frame 11\n",
      "SHAPE (12, 36)\n",
      "dem frame 12\n",
      "SHAPE (13, 36)\n",
      "dem frame 13\n",
      "SHAPE (14, 36)\n",
      "dem frame 14\n",
      "SHAPE (15, 36)\n",
      "dem frame 15\n",
      "SHAPE (16, 36)\n",
      "dem frame 16\n",
      "SHAPE (17, 36)\n",
      "dem frame 17\n",
      "SHAPE (18, 36)\n",
      "dem frame 18\n",
      "SHAPE (19, 36)\n",
      "dem frame 19\n",
      "SHAPE (20, 36)\n",
      "dem frame 20\n",
      "SHAPE (21, 36)\n",
      "dem frame 21\n",
      "SHAPE (22, 36)\n",
      "dem frame 22\n",
      "SHAPE (23, 36)\n",
      "dem frame 23\n",
      "SHAPE (24, 36)\n",
      "dem frame 24\n",
      "SHAPE (25, 36)\n",
      "dem frame 25\n",
      "SHAPE (26, 36)\n",
      "dem frame 26\n",
      "SHAPE (27, 36)\n",
      "dem frame 27\n",
      "SHAPE (28, 36)\n",
      "dem frame 28\n",
      "SHAPE (29, 36)\n",
      "dem frame 29\n",
      "SHAPE (30, 36)\n",
      "dem frame 30\n",
      "SHAPE (31, 36)\n",
      "dem frame 31\n",
      "SHAPE (32, 36)\n",
      "SHAPEPRO (1, 32, 36)\n",
      "SHAPEPRO [[[26.5  21.5  27.5  ... 13.5  32.5  13.5 ]\n",
      "  [26.5  21.5  27.5  ... 13.25 32.5  13.5 ]\n",
      "  [26.5  21.5  27.5  ... 13.5  32.5  13.25]\n",
      "  ...\n",
      "  [26.5  21.25 27.5  ... 13.25 33.25 13.5 ]\n",
      "  [26.5  21.25 27.5  ... 13.5  33.25 14.25]\n",
      "  [26.5  21.5  28.25 ... 13.5  33.25 14.5 ]]]\n",
      "input tensor([[[26.5000, 21.5000, 27.5000,  ..., 13.5000, 32.5000, 13.5000],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.5000],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         ...,\n",
      "         [26.5000, 21.2500, 27.5000,  ..., 13.2500, 33.2500, 13.5000],\n",
      "         [26.5000, 21.2500, 27.5000,  ..., 13.5000, 33.2500, 14.2500],\n",
      "         [26.5000, 21.5000, 28.2500,  ..., 13.5000, 33.2500, 14.5000]]])\n",
      "Tong hop) WAVING_2HANDS 3 tensor([[[26.5000, 21.5000, 27.5000,  ..., 13.5000, 32.5000, 13.5000],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.2500, 32.5000, 13.5000],\n",
      "         [26.5000, 21.5000, 27.5000,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         ...,\n",
      "         [26.5000, 21.2500, 27.5000,  ..., 13.2500, 33.2500, 13.5000],\n",
      "         [26.5000, 21.2500, 27.5000,  ..., 13.5000, 33.2500, 14.2500],\n",
      "         [26.5000, 21.5000, 28.2500,  ..., 13.5000, 33.2500, 14.5000]]],\n",
      "       device='cuda:0')\n",
      "dem frame 0\n",
      "Dautien [[26.5  21.5  28.25 20.5  37.5  21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   15.25 21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   24.5  17.25 31.25 16.5  20.5  14.25 33.5  15.5 ]]\n",
      "SHAPE (1, 36)\n",
      "dem frame 1\n",
      "SHAPE (2, 36)\n",
      "dem frame 2\n",
      "SHAPE (3, 36)\n",
      "dem frame 3\n",
      "SHAPE (4, 36)\n",
      "dem frame 4\n",
      "SHAPE (5, 36)\n",
      "dem frame 5\n",
      "SHAPE (6, 36)\n",
      "dem frame 6\n",
      "SHAPE (7, 36)\n",
      "dem frame 7\n",
      "SHAPE (8, 36)\n",
      "dem frame 8\n",
      "SHAPE (9, 36)\n",
      "dem frame 9\n",
      "SHAPE (10, 36)\n",
      "dem frame 10\n",
      "SHAPE (11, 36)\n",
      "dem frame 11\n",
      "SHAPE (12, 36)\n",
      "dem frame 12\n",
      "SHAPE (13, 36)\n",
      "dem frame 13\n",
      "SHAPE (14, 36)\n",
      "dem frame 14\n",
      "SHAPE (15, 36)\n",
      "dem frame 15\n",
      "SHAPE (16, 36)\n",
      "dem frame 16\n",
      "SHAPE (17, 36)\n",
      "dem frame 17\n",
      "SHAPE (18, 36)\n",
      "dem frame 18\n",
      "SHAPE (19, 36)\n",
      "dem frame 19\n",
      "SHAPE (20, 36)\n",
      "dem frame 20\n",
      "SHAPE (21, 36)\n",
      "dem frame 21\n",
      "SHAPE (22, 36)\n",
      "dem frame 22\n",
      "SHAPE (23, 36)\n",
      "dem frame 23\n",
      "SHAPE (24, 36)\n",
      "dem frame 24\n",
      "SHAPE (25, 36)\n",
      "dem frame 25\n",
      "SHAPE (26, 36)\n",
      "dem frame 26\n",
      "SHAPE (27, 36)\n",
      "dem frame 27\n",
      "SHAPE (28, 36)\n",
      "dem frame 28\n",
      "SHAPE (29, 36)\n",
      "dem frame 29\n",
      "SHAPE (30, 36)\n",
      "dem frame 30\n",
      "SHAPE (31, 36)\n",
      "dem frame 31\n",
      "SHAPE (32, 36)\n",
      "SHAPEPRO (1, 32, 36)\n",
      "SHAPEPRO [[[26.5  21.5  28.25 ... 14.25 33.5  15.5 ]\n",
      "  [26.5  21.5  28.25 ... 14.25 33.5  15.5 ]\n",
      "  [26.5  21.5  28.25 ... 14.25 33.5  15.5 ]\n",
      "  ...\n",
      "  [26.5  21.5  27.25 ... 13.5  32.5  13.25]\n",
      "  [26.5  21.5  27.25 ... 13.5  32.5  13.25]\n",
      "  [26.5  21.25 27.25 ... 13.5  32.5  12.5 ]]]\n",
      "input tensor([[[26.5000, 21.5000, 28.2500,  ..., 14.2500, 33.5000, 15.5000],\n",
      "         [26.5000, 21.5000, 28.2500,  ..., 14.2500, 33.5000, 15.5000],\n",
      "         [26.5000, 21.5000, 28.2500,  ..., 14.2500, 33.5000, 15.5000],\n",
      "         ...,\n",
      "         [26.5000, 21.5000, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         [26.5000, 21.5000, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         [26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 12.5000]]])\n",
      "Tong hop) WAVING_2HANDS 3 tensor([[[26.5000, 21.5000, 28.2500,  ..., 14.2500, 33.5000, 15.5000],\n",
      "         [26.5000, 21.5000, 28.2500,  ..., 14.2500, 33.5000, 15.5000],\n",
      "         [26.5000, 21.5000, 28.2500,  ..., 14.2500, 33.5000, 15.5000],\n",
      "         ...,\n",
      "         [26.5000, 21.5000, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         [26.5000, 21.5000, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         [26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 12.5000]]],\n",
      "       device='cuda:0')\n",
      "dem frame 0\n",
      "Dautien [[26.5  21.25 27.25 19.5  37.25 21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   15.25 21.25  0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   23.5  16.25 29.5  15.5  20.25 13.5  32.5  12.5 ]]\n",
      "SHAPE (1, 36)\n",
      "dem frame 1\n",
      "SHAPE (2, 36)\n",
      "dem frame 2\n",
      "SHAPE (3, 36)\n",
      "dem frame 3\n",
      "SHAPE (4, 36)\n",
      "dem frame 4\n",
      "SHAPE (5, 36)\n",
      "dem frame 5\n",
      "SHAPE (6, 36)\n",
      "dem frame 6\n",
      "SHAPE (7, 36)\n",
      "dem frame 7\n",
      "SHAPE (8, 36)\n",
      "dem frame 8\n",
      "SHAPE (9, 36)\n",
      "dem frame 9\n",
      "SHAPE (10, 36)\n",
      "dem frame 10\n",
      "SHAPE (11, 36)\n",
      "dem frame 11\n",
      "SHAPE (12, 36)\n",
      "dem frame 12\n",
      "SHAPE (13, 36)\n",
      "dem frame 13\n",
      "SHAPE (14, 36)\n",
      "dem frame 14\n",
      "SHAPE (15, 36)\n",
      "dem frame 15\n",
      "SHAPE (16, 36)\n",
      "dem frame 16\n",
      "SHAPE (17, 36)\n",
      "dem frame 17\n",
      "SHAPE (18, 36)\n",
      "dem frame 18\n",
      "SHAPE (19, 36)\n",
      "dem frame 19\n",
      "SHAPE (20, 36)\n",
      "dem frame 20\n",
      "SHAPE (21, 36)\n",
      "dem frame 21\n",
      "SHAPE (22, 36)\n",
      "dem frame 22\n",
      "SHAPE (23, 36)\n",
      "dem frame 23\n",
      "SHAPE (24, 36)\n",
      "dem frame 24\n",
      "SHAPE (25, 36)\n",
      "dem frame 25\n",
      "SHAPE (26, 36)\n",
      "dem frame 26\n",
      "SHAPE (27, 36)\n",
      "dem frame 27\n",
      "SHAPE (28, 36)\n",
      "dem frame 28\n",
      "SHAPE (29, 36)\n",
      "dem frame 29\n",
      "SHAPE (30, 36)\n",
      "dem frame 30\n",
      "SHAPE (31, 36)\n",
      "dem frame 31\n",
      "SHAPE (32, 36)\n",
      "SHAPEPRO (1, 32, 36)\n",
      "SHAPEPRO [[[26.5  21.25 27.25 ... 13.5  32.5  12.5 ]\n",
      "  [26.5  21.25 27.25 ... 13.5  32.5  12.5 ]\n",
      "  [26.5  21.5  27.25 ... 13.5  32.5  13.25]\n",
      "  ...\n",
      "  [26.5  21.25 27.25 ... 13.5  32.5  13.25]\n",
      "  [26.5  21.25 27.5  ... 13.5  33.25 13.25]\n",
      "  [26.25 21.5  27.5  ... 13.5  33.25 13.25]]]\n",
      "input tensor([[[26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 12.5000],\n",
      "         [26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 12.5000],\n",
      "         [26.5000, 21.5000, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         ...,\n",
      "         [26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         [26.5000, 21.2500, 27.5000,  ..., 13.5000, 33.2500, 13.2500],\n",
      "         [26.2500, 21.5000, 27.5000,  ..., 13.5000, 33.2500, 13.2500]]])\n",
      "Tong hop) WAVING_2HANDS 3 tensor([[[26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 12.5000],\n",
      "         [26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 12.5000],\n",
      "         [26.5000, 21.5000, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         ...,\n",
      "         [26.5000, 21.2500, 27.2500,  ..., 13.5000, 32.5000, 13.2500],\n",
      "         [26.5000, 21.2500, 27.5000,  ..., 13.5000, 33.2500, 13.2500],\n",
      "         [26.2500, 21.5000, 27.5000,  ..., 13.5000, 33.2500, 13.2500]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dem frame 0\n",
      "Dautien [[26.25 21.25 27.5  19.25 37.5  21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   15.25 21.5   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.   24.25 15.25 30.25 15.25 20.5  13.5  33.25 12.5 ]]\n",
      "SHAPE (1, 36)\n",
      "dem frame 1\n",
      "SHAPE (2, 36)\n",
      "dem frame 2\n",
      "SHAPE (3, 36)\n",
      "dem frame 3\n",
      "SHAPE (4, 36)\n",
      "dem frame 4\n",
      "SHAPE (5, 36)\n",
      "dem frame 5\n",
      "SHAPE (6, 36)\n",
      "dem frame 6\n",
      "SHAPE (7, 36)\n",
      "dem frame 7\n",
      "SHAPE (8, 36)\n",
      "dem frame 8\n",
      "SHAPE (9, 36)\n",
      "dem frame 9\n",
      "SHAPE (10, 36)\n",
      "dem frame 10\n",
      "SHAPE (11, 36)\n",
      "dem frame 11\n",
      "SHAPE (12, 36)\n",
      "dem frame 12\n",
      "SHAPE (13, 36)\n",
      "dem frame 13\n",
      "SHAPE (14, 36)\n",
      "dem frame 14\n",
      "SHAPE (15, 36)\n",
      "dem frame 15\n",
      "SHAPE (16, 36)\n",
      "dem frame 16\n",
      "SHAPE (17, 36)\n",
      "dem frame 17\n",
      "SHAPE (18, 36)\n",
      "dem frame 18\n",
      "SHAPE (19, 36)\n",
      "dem frame 19\n",
      "SHAPE (20, 36)\n",
      "dem frame 20\n",
      "SHAPE (21, 36)\n",
      "dem frame 21\n",
      "SHAPE (22, 36)\n",
      "dem frame 22\n",
      "SHAPE (23, 36)\n",
      "dem frame 23\n",
      "SHAPE (24, 36)\n",
      "dem frame 24\n",
      "SHAPE (25, 36)\n",
      "dem frame 25\n",
      "SHAPE (26, 36)\n",
      "dem frame 26\n",
      "SHAPE (27, 36)\n",
      "dem frame 27\n",
      "SHAPE (28, 36)\n",
      "dem frame 28\n",
      "SHAPE (29, 36)\n",
      "dem frame 29\n",
      "SHAPE (30, 36)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-71d6f9da8730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mdemframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_provider\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTickCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/har/lightweight-human-pose-estimation-3d-demo.pytorch/modules/input_reader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mwas_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwas_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from modules.input_reader import VideoReader, ImageReader\n",
    "from modules.draw import Plotter3d, draw_poses\n",
    "from modules.parse_poses import parse_poses,get_root_relative_poses\n",
    "from modules.inference_engine_pytorch import InferenceEnginePyTorch\n",
    "net = InferenceEnginePyTorch('human-pose-estimation-3d.pth', 'GPU',)\n",
    "\n",
    "har_addframe = []\n",
    "har_32frame = []\n",
    "def rotate_poses(poses_3d, R, t):\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    for pose_id in range(len(poses_3d)):\n",
    "        pose_3d = poses_3d[pose_id].reshape((-1, 4)).transpose()\n",
    "        pose_3d[0:3, :] = np.dot(R_inv, pose_3d[0:3, :] - t)\n",
    "        poses_3d[pose_id] = pose_3d.transpose().reshape(-1)\n",
    "\n",
    "    return poses_3d\n",
    "frame_provider = VideoReader(0)\n",
    "is_video = True\n",
    "base_height = 256 #args.height_size\n",
    "fx = -1 #args.fx\n",
    "\n",
    "delay = 1\n",
    "esc_code = 27\n",
    "p_code = 112\n",
    "space_code = 32\n",
    "mean_time = 0\n",
    "stride = 8\n",
    "demframe = 0\n",
    "\n",
    "category = ''\n",
    "category_ii = ''\n",
    "for frame in frame_provider:\n",
    "    current_time = cv2.getTickCount()\n",
    "    if frame is None:\n",
    "        break\n",
    "    input_scale = base_height / frame.shape[0]\n",
    "    scaled_img = cv2.resize(frame, dsize=None, fx=input_scale, fy=input_scale)\n",
    "    scaled_img = scaled_img[:, 0:scaled_img.shape[1] - (scaled_img.shape[1] % stride)]  # better to pad, but cut out for demo\n",
    "    if fx < 0:  # Focal length is unknown\n",
    "        fx = np.float32(0.8 * frame.shape[1])\n",
    "    inference_result = net.infer(scaled_img)\n",
    "    poses_3d, poses_2d, features_shape,keypoints = get_root_relative_poses(inference_result)\n",
    "    \n",
    "    \n",
    "    #poses_3d, poses_2d = parse_poses(inference_result, input_scale, stride, fx, is_video)\n",
    "    for pose_id in range(len(poses_2d)):\n",
    "        #print(poses_2d[pose_id])\n",
    "        #print(keypoints[pose_id])\n",
    "        print(\"dem frame\",demframe)\n",
    "        if(demframe==0):\n",
    "            har_addframe = np.array([keypoints[0]])\n",
    "            har_addframe[har_addframe<0] = 0\n",
    "            print(\"Dautien\",har_addframe)\n",
    "            print(\"SHAPE\",har_addframe.shape)\n",
    "        if(demframe>0 and demframe<32):   \n",
    "            har_addframe = np.vstack(([har_addframe, [keypoints[0]]]))\n",
    "            har_addframe[har_addframe<0] = 0\n",
    "            print(\"SHAPE\",har_addframe.shape)\n",
    "        if(demframe==31):\n",
    "            har_32frame=np.array([har_addframe])\n",
    "            print(\"SHAPEPRO\", har_32frame.shape)\n",
    "            print(\"SHAPEPRO\", har_32frame)\n",
    "            inputs=torch.from_numpy(har_32frame)\n",
    "            print(\"input\",inputs)\n",
    "            inputs = inputs.to(device)\n",
    "            model.cuda()\n",
    "            output = model(inputs)\n",
    "            top_n, top_i = output.topk(1)\n",
    "            category_i = top_i[0].item()\n",
    "            category = LABELS[category_i]\n",
    "            category_ii = LABELS.index(category)\n",
    "            print(\"Tong hop)\", category ,category_ii)\n",
    "        \n",
    "        demframe+=1\n",
    "        if(demframe >= 32):\n",
    "            har_addframe =[]\n",
    "            demframe = 0\n",
    "            har_32frame = []\n",
    "        #pose = np.array(poses_2d[pose_id][0:-1]).reshape((-1, 3)).transpose()\n",
    "        #print(poses_2d[pose_id])\n",
    "        #np.append(arr, np.array(pose), axis=0)\n",
    "        #print(len(poses_2d[0]))\n",
    "    #break\n",
    "    edges = []\n",
    "    \n",
    "    \n",
    "    #draw_poses(frame, poses_2d)\n",
    "    current_time = (cv2.getTickCount() - current_time) / cv2.getTickFrequency()\n",
    "    if mean_time == 0:\n",
    "        mean_time = current_time\n",
    "    else:\n",
    "        mean_time = mean_time * 0.95 + current_time * 0.05\n",
    "    cv2.putText(frame, 'FPS: {}'.format(int(1 / mean_time * 10) / 10),\n",
    "                (40, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255))\n",
    "     cv2.putText(frame, category + category_ii,\n",
    "                (40, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255))\n",
    "    cv2.imshow('ICV 3D Human Pose Estimation', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch2trt import torch2trt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(n_joints,n_hidden,n_categories,n_layer)\n",
    "model.load_state_dict(torch.load('lstm_6_bn.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1,32,36)).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.fc = torch.nn.Linear(12,1)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        out = self.fc(inputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.cuda().eval()\n",
    "x = torch.ones((1,12)).cuda()\n",
    "model_trt = torch2trt(a, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://github.com/xieyulai/LSTM-for-Human-Activity-Recognition-using-2D-Pose_Pytorch/blob/master/lstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
