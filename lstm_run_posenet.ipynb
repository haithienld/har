{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "record = False\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if (cap.isOpened() == False): \n",
    "  print(\"Unable to read camera feed\")\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "out = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "\n",
    "while(True):\n",
    "  ret, frame = cap.read()\n",
    "  k = cv2.waitKey(1)\n",
    "\n",
    "  if ret == True: \n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    # press space key to start recording\n",
    "    if k%256 == 32:\n",
    "        record = True\n",
    "\n",
    "    if record:\n",
    "        out.write(frame) \n",
    "\n",
    "    # press q key to close the program\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "  else:\n",
    "     break  \n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the networks inputs\n",
    "\n",
    "# Useful Constants\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "n_steps = 32 # 32 timesteps per series\n",
    "\n",
    "def load_X(X_path):\n",
    "    file = open(X_path, 'r')\n",
    "    X_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.split(',') for row in file\n",
    "        ]], \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    file.close()\n",
    "    blocks = int(len(X_) / n_steps)\n",
    "    \n",
    "    X_ = np.array(np.split(X_,blocks))\n",
    "\n",
    "    return X_ \n",
    "\n",
    "# Load the networks outputs\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # for 0-based indexing \n",
    "    return y_ - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,layer_num):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim,hidden_dim,layer_num,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        self.bn = nn.BatchNorm1d(32)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = self.bn(inputs)\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[307.589 162.976 319.364 205.944 293.267 204.68  285.434 250.281 277.616\n",
      "  286.729 349.276 208.539 357.095 255.523 349.362 297.17  297.183 290.662\n",
      "  307.599 351.938 308.905 401.454 329.758 291.958 333.701 357.132 337.546\n",
      "  411.953 304.994 160.291 315.459 160.269   0.      0.    329.752 161.651]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[307.5890, 162.9760, 319.3640,  ...,   0.0000, 329.7520, 161.6510],\n",
       "         [307.5670, 162.9790, 319.3620,  ...,   0.0000, 328.5270, 161.6550],\n",
       "         [306.2980, 162.9510, 319.3510,  ...,   0.0000, 328.4950, 161.6810],\n",
       "         ...,\n",
       "         [293.2910, 122.5340, 307.6760,  ..., 128.9530, 315.4380, 119.8840],\n",
       "         [289.3920, 140.7430, 307.6150,  ...,   0.0000, 315.3930, 139.4080],\n",
       "         [295.8480, 161.6580, 307.6280,  ..., 160.3310, 314.1120, 160.2640]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[307.589,162.976,319.364,205.944,293.267,204.68,285.434,250.281,277.616,286.729,349.276,208.539,357.095,255.523,349.362,297.17,297.183,290.662,307.599,351.938,308.905,401.454,329.758,291.958,333.701,357.132,337.546,411.953,304.994,160.291,315.459,160.269,0,0,329.752,161.651]])    \n",
    "np.r_[arr,[[307.567,162.979,319.362,205.947,293.257,204.695,285.428,250.28,277.616,285.504,349.282,208.541,357.101,255.503,349.361,297.149,297.182,290.671,307.603,351.914,307.689,401.429,329.757,291.969,333.698,357.127,337.545,411.937,304.969,160.295,315.434,160.268,0,0,328.527,161.655\n",
    "]]]\n",
    "print (arr)\n",
    "inputs = load_X(\"demofile3.txt\")\n",
    "inputs=torch.from_numpy(inputs)\n",
    "inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "n_joints = 18*2\n",
    "n_categories = 6\n",
    "n_layer = 3\n",
    "#model = torch.load('lstm_6_bn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(36, 128, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       "  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(n_joints,n_hidden,n_categories,n_layer)\n",
    "model.load_state_dict(torch.load('lstm_6_bn.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BOXING',\n",
       " 2,\n",
       " tensor([[[307.5890, 162.9760, 319.3640,  ...,   0.0000, 329.7520, 161.6510],\n",
       "          [307.5670, 162.9790, 319.3620,  ...,   0.0000, 328.5270, 161.6550],\n",
       "          [306.2980, 162.9510, 319.3510,  ...,   0.0000, 328.4950, 161.6810],\n",
       "          ...,\n",
       "          [293.2910, 122.5340, 307.6760,  ..., 128.9530, 315.4380, 119.8840],\n",
       "          [289.3920, 140.7430, 307.6150,  ...,   0.0000, 315.3930, 139.4080],\n",
       "          [295.8480, 161.6580, 307.6280,  ..., 160.3310, 314.1120, 160.2640]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#category_tensor, inputs = randomTrainingExampleBatch(1,'test',0)\n",
    "#category = LABELS[int(category_tensor[0])]\n",
    "inputs = inputs.to(device)\n",
    "model.cuda()\n",
    "output = model(inputs)\n",
    "top_n, top_i = output.topk(1)\n",
    "category_i = top_i[0].item()\n",
    "category = LABELS[category_i]\n",
    "category_ii = LABELS.index(category)\n",
    "category ,category_ii,inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "#import argparse\n",
    "\n",
    "import posenet\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--model', type=int, default=101)\n",
    "#parser.add_argument('--cam_id', type=int, default=0)\n",
    "#parser.add_argument('--cam_width', type=int, default=1280)\n",
    "#parser.add_argument('--cam_height', type=int, default=720)\n",
    "#parser.add_argument('--scale_factor', type=float, default=0.7125)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "model = posenet.load_model(int(101))#args.model\n",
    "model = model.cuda()\n",
    "output_stride = model.output_stride\n",
    "\n",
    "cap = cv2.VideoCapture(0)#args.cam_id = 0\n",
    "cap.set(3, 1280)#args.cam_width=1280\n",
    "cap.set(4, 720)#args.cam_height=720\n",
    "\n",
    "start = time.time()\n",
    "frame_count = 0\n",
    "while True:\n",
    "    input_image, display_image, output_scale = posenet.read_cap(\n",
    "        cap, scale_factor= 0.7125, output_stride=output_stride)#args.scale_factor=0.7125\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_image = torch.Tensor(input_image).cuda()\n",
    "\n",
    "        heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = model(input_image)\n",
    "\n",
    "        pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n",
    "            heatmaps_result.squeeze(0),\n",
    "            offsets_result.squeeze(0),\n",
    "            displacement_fwd_result.squeeze(0),\n",
    "            displacement_bwd_result.squeeze(0),\n",
    "            output_stride=output_stride,\n",
    "            max_pose_detections=10,\n",
    "            min_pose_score=0.15)\n",
    "\n",
    "    keypoint_coords *= output_scale\n",
    "\n",
    "    # TODO this isn't particularly fast, use GL for drawing and display someday...\n",
    "    overlay_image = posenet.draw_skel_and_kp(\n",
    "        display_image, pose_scores, keypoint_scores, keypoint_coords,\n",
    "        min_pose_score=0.15, min_part_score=0.1)\n",
    "\n",
    "    cv2.imshow('posenet', overlay_image)\n",
    "    frame_count += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    print('Average FPS: ', frame_count / (time.time() - start))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch2trt import torch2trt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(n_joints,n_hidden,n_categories,n_layer)\n",
    "model.load_state_dict(torch.load('lstm_6_bn.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1,32,36)).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.fc = torch.nn.Linear(12,1)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        out = self.fc(inputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.cuda().eval()\n",
    "x = torch.ones((1,12)).cuda()\n",
    "model_trt = torch2trt(a, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://github.com/xieyulai/LSTM-for-Human-Activity-Recognition-using-2D-Pose_Pytorch/blob/master/lstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
